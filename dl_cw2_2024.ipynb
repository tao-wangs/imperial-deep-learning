{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzzPw7rEZ-OX"
      },
      "source": [
        "# Coursework 2: Generative Models\n",
        "\n",
        "## Instructions\n",
        "\n",
        "### Submission \n",
        "Please submit one zip file on cate - *CW2.zip* containing the following:\n",
        "1. A version of this notebook containing your answers. Write your answers in the cells below each question. **Please deliver the notebook including the outputs of the cells**\n",
        "2. Your trained VAE model as *VAE_model.pth*\n",
        "3. Your trained Generator and Discriminator: *DCGAN_model_D.pth and DCGAN_model_G.pth*\n",
        "\n",
        "\n",
        "### Training\n",
        "Training the GAN will take quite a long time (multiple hours), please refer to the 4 GPU options detailed in the logistics lecture. Some additional useful pointers:\n",
        "* PaperSpace [guide if you need more compute](https://hackmd.io/@afspies/S1stL8Qnt)\n",
        "* Lab GPUs via SSH.  The VSCode Remote Develop extension is recommended for this. For general Imperial remote working instructions see [this post](https://www.doc.ic.ac.uk/~nuric/teaching/remote-working-for-imperial-computing-students.html). You'll also want to [setup your environment as outlined here](https://hackmd.io/@afspies/Bkd7Zq60K).\n",
        "* Use Colab and add checkpointing to the model training code; this is to handle the case where colab stops a free-GPU kernel after a certain number of hours (~4).\n",
        "* Use Colab Pro - If you do not wish to use PaperSpace then you can pay for Colab Pro. We cannot pay for this on your behalf (this is Google's fault).\n",
        "\n",
        "\n",
        "### Testing\n",
        "TAs will run a testing cell (at the end of this notebook), so you are required to copy your data ```transform``` and ```denorm``` functions to a cell near the bottom of the document (it is demarkated). You are advised to check that your implementations pass these tests (in particular, the jit saving and loading may not work for certain niche functions)\n",
        "\n",
        "### General\n",
        "You can feel free to add architectural alterations / custom functions outside of pre-defined code blocks, but if you manipulate the model's inputs in some way, please include the same code in the TA test cell, so our tests will run easily.\n",
        "\n",
        "<font color=\"orange\">**The deadline for submission is Monday, 26 Feb by 6 pm** </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oqY55OLpxDm"
      },
      "source": [
        "## Setting up working environment\n",
        "You will need to install pytorch and import some utilities by running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "FJg7ozC_q3HF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
            "[notice] To update, run: C:\\Users\\taow\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
            "fatal: destination path 'icl_dl_cw2_utils' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch torchvision altair seaborn\n",
        "!git clone -q https://github.com/afspies/icl_dl_cw2_utils\n",
        "from icl_dl_cw2_utils.utils.plotting import plot_tsne\n",
        "from pathlib import Path\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwTHVWdabZhZ"
      },
      "source": [
        "Here we have some default pathing options which vary depending on the environment you are using. You can of course change these as you please."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialization Cell\n",
        "WORKING_ENV = 'PERSONAL' # Can be LABS, COLAB, PAPERSPACE, SAGEMAKER\n",
        "USERNAME = 'tw520' # If working on Lab Machines - Your college username\n",
        "assert WORKING_ENV in ['LABS', 'COLAB', 'PAPERSPACE', 'SAGEMAKER', 'PERSONAL']\n",
        "\n",
        "if WORKING_ENV == 'COLAB':\n",
        "    from google.colab import drive\n",
        "    %load_ext google.colab.data_table\n",
        "    dl_cw2_repo_path = 'dl_cw2/' # path in your gdrive to the repo\n",
        "    content_path = f'/content/drive/MyDrive/{dl_cw2_repo_path}' # path to gitrepo in gdrive after mounting\n",
        "    data_path = './data/' # save the data locally\n",
        "    drive.mount('/content/drive/') # Outputs will be saved in your google drive\n",
        "\n",
        "elif WORKING_ENV == 'LABS':\n",
        "    content_path = f'/vol/bitbucket/{USERNAME}/dl/dl_cw2/' # You may want to change this\n",
        "    data_path = f'/vol/bitbucket/{USERNAME}/dl/'\n",
        "    # Your python env and training data should be on bitbucket\n",
        "    if 'vol/bitbucket' not in content_path or 'vol/bitbucket' not in data_path:\n",
        "        import warnings\n",
        "        warnings.warn(\n",
        "           'It is best to create a dir in /vol/bitbucket/ otherwise you will quickly run into memory issues'\n",
        "           )\n",
        "elif WORKING_ENV == 'PAPERSPACE': # Using Paperspace\n",
        "    # Paperspace does not properly render animated progress bars\n",
        "    # Strongly recommend using the JupyterLab UI instead of theirs\n",
        "    !pip install ipywidgets\n",
        "    content_path = '/notebooks/'\n",
        "    data_path = './data/'\n",
        "    \n",
        "elif WORKING_ENV == 'SAGEMAKER':\n",
        "    content_path = '/home/studio-lab-user/sagemaker-studiolab-notebooks/dl/'\n",
        "    data_path = f'{content_path}data/'\n",
        "\n",
        "elif WORKING_ENV == 'PERSONAL':\n",
        "    content_path = ''\n",
        "    data_path = f'{content_path}data/'\n",
        "else:\n",
        "  raise NotImplementedError()\n",
        "\n",
        "content_path = Path(content_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezLSfB6IqAzK"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "For this coursework, you are asked to implement two commonly used generative models:\n",
        "1. A **Variational Autoencoder (VAE)**\n",
        "2. A **Deep Convolutional Generative Adversarial Network (DCGAN)**\n",
        "\n",
        "For the first part you will the MNIST dataset https://en.wikipedia.org/wiki/MNIST_database and for the second the CIFAR-10 (https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "Each part is worth 50 points. \n",
        "\n",
        "The emphasis of both parts lies in understanding how the models behave and learn, however, some points will be available for getting good results with your GAN (though you should not spend too long on this)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75mICbvzqQyx"
      },
      "source": [
        "# Part 1 - Variational Autoencoder\n",
        "\n",
        "## Part 1.1 (25 points)\n",
        "**Your Task:**\n",
        "\n",
        "a. Implement the VAE architecture with accompanying hyperparameters. More marks are awarded for using a Convolutional Encoder and Decoder.\n",
        "\n",
        "b. Design an appropriate loss function and train the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ym5l5RmmJtLw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, sampler\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show(img):\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
        "\n",
        "if not os.path.exists(content_path/'CW_VAE/'):\n",
        "    os.makedirs(content_path/'CW_VAE/')\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    os.makedirs(data_path)\n",
        "\n",
        "# We set a random seed to ensure that your results are reproducible.\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(0)\n",
        "\n",
        "GPU = True # Choose whether to use GPU\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f'Using {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqT7sdGzJtLy"
      },
      "source": [
        "---\n",
        "## Part 1.1a: Implement VAE (25 Points)\n",
        "### Hyper-parameter selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ZVPM6pgqJtLz"
      },
      "outputs": [],
      "source": [
        "# Necessary Hyperparameters \n",
        "num_epochs = 10\n",
        "learning_rate = 0.0001 \n",
        "batch_size = 128\n",
        "latent_dim = 50 # Choose a value for the size of the latent space\n",
        "\n",
        "# Additional Hyperparameters \n",
        "beta = 0.8\n",
        "\n",
        "\n",
        "# (Optionally) Modify transformations on input\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# (Optionally) Modify the network's output for visualizing your images\n",
        "def denorm(x):\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN5aL7sdJtL2"
      },
      "source": [
        "### Data loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "VOKdAZa2JtL3"
      },
      "outputs": [],
      "source": [
        "train_dat = datasets.MNIST(\n",
        "    data_path, train=True, download=True, transform=transform\n",
        ")\n",
        "test_dat = datasets.MNIST(data_path, train=False, transform=transform)\n",
        "\n",
        "loader_train = DataLoader(train_dat, batch_size, shuffle=True)\n",
        "loader_test = DataLoader(test_dat, batch_size, shuffle=False)\n",
        "\n",
        "# Don't change \n",
        "sample_inputs, _ = next(iter(loader_test))\n",
        "fixed_input = sample_inputs[:32, :, :, :]\n",
        "save_image(fixed_input, content_path/'CW_VAE/image_original.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiQDXD24JtL7"
      },
      "source": [
        "### Model Definition\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://blog.bayeslabs.co/assets/img/vae-gaussian.png\" style=\"width:60%\">\n",
        "  <figcaption>\n",
        "    Fig.1 - VAE Diagram (with a Guassian prior), taken from <a href=\"https://blog.bayeslabs.co/2019/06/04/All-you-need-to-know-about-Vae.html\">1</a>.\n",
        "  </figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "You will need to define:\n",
        "* The hyperparameters\n",
        "* The constructor\n",
        "* encode\n",
        "* reparametrize\n",
        "* decode\n",
        "* forward\n",
        "\n",
        "\n",
        "\n",
        "Hints:\n",
        "- It is common practice to encode the log of the variance, rather than the variance\n",
        "- You might try using BatchNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "wDlll3BUJtL8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters is: 540583\n",
            "VAE(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (fc_mu): Linear(in_features=3136, out_features=50, bias=True)\n",
            "  (fc_logvar): Linear(in_features=3136, out_features=50, bias=True)\n",
            "  (fc1): Linear(in_features=50, out_features=3136, bias=True)\n",
            "  (conv3): Sequential(\n",
            "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# *CODE FOR PART 1.1a IN THIS CELL*\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        #######################################################################\n",
        "        #                       ** START OF YOUR CODE **\n",
        "        #######################################################################\n",
        "        # encoder\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=32, \n",
        "                                             kernel_size=4, stride=2, padding=1),\n",
        "                                   nn.BatchNorm2d(32),\n",
        "                                   nn.ReLU(inplace=True))\n",
        "        \n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels=32, out_channels=64, \n",
        "                                             kernel_size=4, stride=2, padding=1),\n",
        "                                   nn.BatchNorm2d(64),\n",
        "                                   nn.ReLU(inplace=True))\n",
        "        \n",
        "        self.fc_mu = nn.Linear(64*7*7, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(64*7*7, latent_dim)\n",
        "        \n",
        "        # decoder\n",
        "        self.fc1 = nn.Linear(latent_dim, 64*7*7)\n",
        "        \n",
        "        self.conv3 = nn.Sequential(nn.ConvTranspose2d(64, 32, kernel_size=4, \n",
        "                                                      stride=2, padding=1),\n",
        "                                   nn.BatchNorm2d(32),\n",
        "                                   nn.ReLU(inplace=True))\n",
        "        \n",
        "        self.conv4 = nn.Sequential(nn.ConvTranspose2d(32, 1, kernel_size=4, \n",
        "                                                      stride=2, padding=1),\n",
        "                                   nn.BatchNorm2d(1),\n",
        "                                   nn.ReLU(inplace=True))\n",
        "\n",
        "        #######################################################################\n",
        "        #                       ** END OF YOUR CODE **\n",
        "        ####################################################################### \n",
        "        \n",
        "    def encode(self, x):\n",
        "        #######################################################################\n",
        "        #                       ** START OF YOUR CODE **\n",
        "        #######################################################################\n",
        "        x = self.conv1(x)\n",
        "        # print('Conv1 done')\n",
        "        # print(x.shape)\n",
        "        x = self.conv2(x)\n",
        "        # print('Conv2 done')\n",
        "        # print(x.shape)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        # print('Reshape done')\n",
        "        # print(x.shape)\n",
        "        \n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "\n",
        "        return mu, logvar\n",
        "        #######################################################################\n",
        "        #                       ** END OF YOUR CODE **\n",
        "        ####################################################################### \n",
        "    \n",
        "    def reparametrize(self, mu, logvar):\n",
        "        #######################################################################\n",
        "        #                       ** START OF YOUR CODE **\n",
        "        #######################################################################\n",
        "        # calculate standard deviation\n",
        "        sd = torch.exp(0.5 * logvar)\n",
        "        # sample noise from standard Gaussian\n",
        "        eps = torch.randn_like(sd)\n",
        "        return mu + eps * sd\n",
        "\n",
        "        #######################################################################\n",
        "        #                       ** END OF YOUR CODE **\n",
        "        ####################################################################### \n",
        "\n",
        "    def decode(self, z):\n",
        "        #######################################################################\n",
        "        #                       ** START OF YOUR CODE **\n",
        "        #######################################################################\n",
        "        # print('Before linear 1')\n",
        "        # print(z.shape)\n",
        "        z = self.fc1(z)\n",
        "        # print('After linear 1')\n",
        "        # print(z.shape)\n",
        "        z = z.view(-1, 64, 7, 7)\n",
        "        z = self.conv3(z)\n",
        "        z = self.conv4(z)\n",
        "\n",
        "        return z\n",
        "        #######################################################################\n",
        "        #                       ** END OF YOUR CODE **\n",
        "        ####################################################################### \n",
        "    \n",
        "    def forward(self, x):\n",
        "        #######################################################################\n",
        "        #                       ** START OF YOUR CODE **\n",
        "        #######################################################################\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparametrize(mu, logvar)\n",
        "        y = self.decode(z)\n",
        "\n",
        "        return y, mu, logvar\n",
        "        #######################################################################\n",
        "        #                       ** END OF YOUR CODE **\n",
        "        ####################################################################### \n",
        "\n",
        "model = VAE(latent_dim).to(device)\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of parameters is: {}\".format(params))\n",
        "print(model)\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeSX6RZhJtMB"
      },
      "source": [
        "--- \n",
        "\n",
        "## Part 1.1b: Training the Model (5 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN-Pc0mvq-7_"
      },
      "source": [
        "### Defining a Loss\n",
        "Recall the Beta VAE loss, with an encoder $q$ and decoder $p$:\n",
        "$$ \\mathcal{L}=\\mathbb{E}_{q_\\phi(z \\mid X)}[\\log p_\\theta(X \\mid z)]-\\beta D_{K L}[q_\\phi(z \\mid X) \\| p_\\theta(z)]$$\n",
        "\n",
        "In order to implement this loss you will need to think carefully about your model's outputs and the choice of prior.\n",
        "\n",
        "There are multiple accepted solutions. Explain your design choices based on the assumptions you make regarding the distribution of your data.\n",
        "\n",
        "* Hint: this refers to the log likelihood as mentioned in the tutorial. Make sure these assumptions reflect on the values of your input data, i.e. depending on your choice you might need to do a simple preprocessing step.\n",
        "\n",
        "* You are encouraged to experiment with the weighting coefficient $\\beta$ and observe how it affects your training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "F6CeeS9CJtMC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10:  59%|█████▉    | 279/469 [00:17<00:11, 15.91batch/s, loss=0.592]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[52], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss, total_kld_loss, total_reconstruction_loss, data\n\u001b[0;32m     85\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 86\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[52], line 72\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch, model, beta)\u001b[0m\n\u001b[0;32m     69\u001b[0m total_reconstruction_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reconstruction_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 72\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# *CODE FOR PART 1.1b IN THIS CELL*\n",
        "\n",
        "def loss_function_VAE(recon_x, x, mu, logvar, beta):\n",
        "        #######################################################################\n",
        "        #                       ** START OF YOUR CODE **\n",
        "        #######################################################################\n",
        "        reconstruction_loss = F.mse_loss(recon_x, x, size_average=False, reduction='sum')\n",
        "        # kld_loss not right\n",
        "        kld_loss = torch.sum(torch.exp(logvar) + mu**2 - logvar - 1.0) * 0.5\n",
        "        loss = reconstruction_loss + beta*kld_loss\n",
        "\n",
        "        return kld_loss, reconstruction_loss, loss\n",
        "        #######################################################################\n",
        "        #                       ** END OF YOUR CODE **\n",
        "        ####################################################################### \n",
        "\n",
        "# model.train()\n",
        "# # <- You may wish to add logging info here\n",
        "# for epoch in range(num_epochs):  \n",
        "#     # <- You may wish to add logging info here\n",
        "#     with tqdm.tqdm(loader_train, unit=\"batch\") as tepoch: \n",
        "#         for batch_idx, (data, _) in enumerate(tepoch):   \n",
        "#             #######################################################################\n",
        "#             #                       ** START OF YOUR CODE **\n",
        "#             #######################################################################\n",
        "#             data = None # Need at least one batch/random data with right shape -\n",
        "#                         # This is required to initialize to model properly below\n",
        "#                         # when we save the computational graph for testing (jit.save)\n",
        "#             loss = None\n",
        "#             #######################################################################\n",
        "#             #                       ** END OF YOUR CODE **\n",
        "#             ####################################################################### \n",
        "#             if batch_idx % 20 == 0:\n",
        "#                 tepoch.set_description(f\"Epoch {epoch}\")\n",
        "#                 tepoch.set_postfix(loss=loss.item()/len(data))\n",
        "\n",
        "#     # save the model\n",
        "#     if epoch == num_epochs - 1:\n",
        "#         with torch.no_grad():\n",
        "#             torch.jit.save(torch.jit.trace(model, (data), check_trace=False),\n",
        "#                 content_path/'CW_VAE/VAE_model.pth')\n",
        "\n",
        "# TEMP TEMP TEMP TEMP TEMP TEMP TEMP\n",
        "def train(epoch, model, beta=1):\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    total_kld_loss = 0\n",
        "    total_reconstruction_loss = 0\n",
        "    \n",
        "    with tqdm.tqdm(loader_train, unit=\"batch\") as tepoch: \n",
        "        for batch_idx, (data, _) in enumerate(tepoch):   \n",
        "            \n",
        "            x = data.to(device)\n",
        "\n",
        "            data = data # Need at least one batch/random data with right shape -\n",
        "                        # This is required to initialize to model properly below\n",
        "                        # when we save the computational graph for testing (jit.save)\n",
        "\n",
        "            recon_x, mu, logvar= model.forward(x)\n",
        "            kld_loss, reconstruction_loss, loss = loss_function_VAE(recon_x, x, mu, logvar, beta)\n",
        "            \n",
        "            # divide by the batch size\n",
        "            loss = loss / data.shape[0]\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            total_kld_loss += kld_loss.item() / data.shape[0]\n",
        "            total_reconstruction_loss += reconstruction_loss.item() / data.shape[0]\n",
        "                \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if batch_idx % 20 == 0:\n",
        "                tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                tepoch.set_postfix(loss=loss.item()/len(data))\n",
        "    \n",
        "    total_loss /= len(loader_train.dataset)\n",
        "    total_kld_loss /= len(loader_train.dataset)\n",
        "    total_reconstruction_loss /= len(loader_train.dataset)\n",
        "    \n",
        "    return total_loss, total_kld_loss, total_reconstruction_loss, data\n",
        "\n",
        "model.train()\n",
        "train(num_epochs, model, beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF6B26_oJtMF"
      },
      "source": [
        "### Loss Explanation\n",
        "Explain your choice of loss and how this relates to:\n",
        "\n",
        "* The VAE Prior\n",
        "* The output data domain\n",
        "* Disentanglement in the latent space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUqWwUvlrYnH"
      },
      "outputs": [],
      "source": [
        "# Any code for your explanation here (you may not need to use this cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhjE07mrB7Zs"
      },
      "source": [
        "**YOUR ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez5nlMi1JtMF"
      },
      "source": [
        "<h2>Part 1.2 (9 points)</h2>\n",
        "\n",
        "a. Plot your loss curves\n",
        "\n",
        "b. Show reconstructions and samples\n",
        "\n",
        "c. Discuss your results from parts (a) and (b)\n",
        "\n",
        "## Part 1.2a: Loss Curves (3 Points)\n",
        "Plot your loss curves (6 in total, 3 for the training set and 3 for the test set): total loss, reconstruction log likelihood loss, KL loss (x-axis: epochs, y-axis: loss). If you experimented with different values of $\\beta$, you may wish to display multiple plots (worth 1 point)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AADYspqtJtMG"
      },
      "outputs": [],
      "source": [
        "# *CODE FOR PART 1.2a IN THIS CELL*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7wp4MzqsjjZ"
      },
      "source": [
        "## Part 1.2b: Samples and Reconstructions (6 Points)\n",
        "Visualize a subset of the images of the test set and their reconstructions **as well as** a few generated samples. Most of the code for this part is provided. You only need to call the forward pass of the model for the given inputs (might vary depending on your implementation).\n",
        "\n",
        "For reference, here's [some samples from our VAE](https://imgur.com/NwNMuG3).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wu9CWtqoJtMK"
      },
      "outputs": [],
      "source": [
        "# *CODE FOR PART 1.2b IN THIS CELL*\n",
        "\n",
        "# load the model\n",
        "print('Input images')\n",
        "print('-'*50)\n",
        "\n",
        "sample_inputs, _ = next(iter(loader_test))\n",
        "fixed_input = sample_inputs[0:32, :, :, :]\n",
        "# visualize the original images of the last batch of the test set\n",
        "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
        "                value_range=None, scale_each=False, pad_value=0)\n",
        "plt.figure()\n",
        "show(img)\n",
        "\n",
        "print('Reconstructed images')\n",
        "print('-'*50)\n",
        "with torch.no_grad():\n",
        "    # visualize the reconstructed images of the last batch of test set\n",
        "    \n",
        "    #######################################################################\n",
        "    #                       ** START OF YOUR CODE **\n",
        "    #######################################################################\n",
        "    recon_batch = \n",
        "    #######################################################################\n",
        "    #                       ** END OF YOUR CODE **\n",
        "    ####################################################################### \n",
        "    \n",
        "    recon_batch = recon_batch.cpu()\n",
        "    recon_batch = make_grid(denorm(recon_batch), nrow=8, padding=2, normalize=False,\n",
        "                            value_range=None, scale_each=False, pad_value=0)\n",
        "    plt.figure()\n",
        "    show(recon_batch)\n",
        "\n",
        "print('Generated Images')  \n",
        "print('-'*50)\n",
        "model.eval()\n",
        "n_samples = 256\n",
        "z = torch.randn(n_samples,latent_dim).to(device)\n",
        "with torch.no_grad():\n",
        "    #######################################################################\n",
        "    #                       ** START OF YOUR CODE **\n",
        "    #######################################################################\n",
        "    samples =\n",
        "    #######################################################################\n",
        "    #                       ** END OF YOUR CODE **\n",
        "    ####################################################################### \n",
        "    \n",
        "    samples = samples.cpu()\n",
        "    samples = make_grid(denorm(samples), nrow=16, padding=2, normalize=False,\n",
        "                            value_range=None, scale_each=False, pad_value=0)\n",
        "    plt.figure(figsize = (8,8))\n",
        "    show(samples)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfZJRgj1rudZ"
      },
      "source": [
        "### Discussion\n",
        "Provide a brief analysis of your loss curves and reconstructions: \n",
        "* What do you observe in the behaviour of the log-likelihood loss and the KL loss (increasing/decreasing)?\n",
        "* Can you intuitively explain if this behaviour is desirable? \n",
        "* What is posterior collapse and did you observe it during training (i.e. when the KL is too small during the early stages of training)? \n",
        "    * If yes, how did you mitigate it? How did this phenomenon reflect on your output samples?\n",
        "    * If no, why do you think that is?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy4KKp2UJtMJ"
      },
      "source": [
        "**YOUR ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTprojS7sLP8"
      },
      "source": [
        "---\n",
        "<h2> Part 1.3 (11 points) <h2/>\n",
        "\n",
        "Qualitative analysis of the learned representations\n",
        "\n",
        "In this question you are asked to qualitatively assess the representations that your model has learned. In particular:\n",
        "\n",
        "a. Dimensionality Reduction of learned embeddings\n",
        "\n",
        "b. Interpolating in the latent space\n",
        "\n",
        "## Part 1.3a: T-SNE on Embeddings (7 Points)\n",
        "Extract the latent representations of the test set and visualize them using [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)  [(see implementation)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). You can use a T-SNE implementation from a library such as scikit-learn. \n",
        "\n",
        "We've provided a function to visualize a subset of the data, but you are encouraged to also produce a matplotlib plot (please use different colours for each digit class)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl4xZOg7s0ke",
        "outputId": "4af471ef-cb1c-4dee-d80e-0ef20981f5d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing TSNE\n"
          ]
        }
      ],
      "source": [
        "# *CODE FOR PART 1.3a IN THIS CELL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M_EI2ZnnXHZ"
      },
      "outputs": [],
      "source": [
        "# Interactive Visualization - Code Provided\n",
        "test_dataloader = DataLoader(test_dat, 10000, shuffle=False)\n",
        "\"\"\" Inputs to the function are\n",
        "        z_embedded - Embedded X, Y positions for every point in test_dataloader\n",
        "        test_dataloader - dataloader with batchsize set to 10000\n",
        "        num_points - number of points plotted (will slow down with >1k)\n",
        "\"\"\"\n",
        "plot_tsne(z_embedded, test_dataloader, num_points=1000, darkmode=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvQvtlDzIB3M"
      },
      "outputs": [],
      "source": [
        "# Custom Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HiAHb0ztTW8"
      },
      "source": [
        "### Discussion\n",
        "What do you observe? Discuss the structure of the visualized representations. \n",
        "* What do you observe? What role do the KL loss term and $\\beta$ have, if any, in what you observe (multiple matplotlib plots may be desirable here)?\n",
        "    * Consider Outliers\n",
        "    * Counsider Boundaries\n",
        "    * Consider Clusters\n",
        "* Is T-SNE reliable? What happens if you change the parameters (don't worry about being particularly thorough). [This link](https://distill.pub/2016/misread-tsne/) may be helpful.\n",
        "\n",
        "Note - If you created multiple plots and want to include them in your discussion, the best option is to upload them to (e.g.) google drive and then embed them via a **public** share link. If you reference local files, please include these in your submission zip, and use relative pathing if you are embedding them (with the notebook in the base directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0_3QlEYteYk"
      },
      "source": [
        "**YOUR ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCtbTLv4thEH"
      },
      "source": [
        "## Part 1.3b: Interpolating in $z$ (4 Points)\n",
        "Perform a linear interpolation in the latent space of the autoencoder by choosing any two digits from the test set. What do you observe regarding the transition from on digit to the other?\n",
        "\n",
        "_hint: Locate the positions in latent space of 2 data points (maybe a one and an eight). Then sample multiple latent space vectors along the line which joins the 2 points and pass them through the decoder._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVk7GUIxtgiF"
      },
      "outputs": [],
      "source": [
        "# CODE FOR PART 1.3b IN THIS CELL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdk6yyrittNx"
      },
      "source": [
        "### Discussion\n",
        "What did you observe in the interpolation? Is this what you expected?\n",
        "* Can you relate the interpolation to your T-SNE visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF2jUHWHtt3V"
      },
      "source": [
        "**YOUR ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG68ntJ2qfIC"
      },
      "source": [
        "# Part 2 - Deep Convolutional GAN\n",
        "\n",
        "In this task, your main objective is to train a DCGAN (https://arxiv.org/abs/1511.06434) on the CIFAR-10 dataset. You should experiment with different architectures and tricks for stability in training (such as using different activation functions, batch normalization, different values for the hyper-parameters, etc.). In the end, you should provide us with: \n",
        "\n",
        "- your best trained model (which we will be able to load and run), \n",
        "- some generations for the fixed latent vectors $\\mathbf{z}\\sim \\mathcal{N}\\left(\\mathbf{0}, \\mathbf{I}\\right)$ we have provided you with (train for a number of epochs and make sure there is no mode collapse), \n",
        "- plots with the losses for the discriminator $D$ and the generator $G$ as the training progresses and explain whether your produced plots are theoretically sensible and why this is (or not) the case. \n",
        "- a discussion on whether you noticed any mode collapse, where this behaviour may be attributed to, and explanations of what you did in order to cope with mode collapse. \n",
        "\n",
        "## Part 2.1 (30 points)\n",
        "**Your Task**: \n",
        "\n",
        "a. Implement the DCGAN architecture. \n",
        "\n",
        "b. Define a loss and implement the Training Loop\n",
        "\n",
        "c. Visualize images sampled from your best model's generator (\"Extension\" Assessed on quality)\n",
        "\n",
        "d. Discuss the experimentations which led to your final architecture. You can plot losses or generated results by other architectures that you tested to back your arguments (but this is not necessary to get full marks).\n",
        "\n",
        "\n",
        "_Clarification: You should not be worrying too much about getting an \"optimal\" performance on your trained GAN. We want you to demonstrate to us that you experimented with different types of DCGAN variations, report what difficulties transpired throughout the training process, etc. In other words, if we see that you provided us with a running implementation, that you detail different experimentations that you did before providing us with your best one, and that you have grapsed the concepts, you can still get good marks. The attached model does not have to be perfect, and the extension marks for performance are only worth 10 points._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFEt7wGXP_aE",
        "outputId": "a93f75c8-8a22-4386-8d0d-ddb799565d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f66859a1bb8>"
            ]
          },
          "execution_count": 15,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mean = torch.Tensor([0.4914, 0.4822, 0.4465])\n",
        "std = torch.Tensor([0.247, 0.243, 0.261])\n",
        "unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
        "\n",
        "def denorm(x, channels=None, w=None ,h=None, resize = False):\n",
        "    \n",
        "    x = unnormalize(x)\n",
        "    if resize:\n",
        "        if channels is None or w is None or h is None:\n",
        "            print('Number of channels, width and height must be provided for resize.')\n",
        "        x = x.view(x.size(0), channels, w, h)\n",
        "    return x\n",
        "\n",
        "def show(img):\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
        "\n",
        "if not os.path.exists(content_path/'CW_GAN'):\n",
        "    os.makedirs(content_path/'CW_GAN')\n",
        "\n",
        "GPU = True # Choose whether to use GPU\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f'Using {device}')\n",
        "\n",
        "# We set a random seed to ensure that your results are reproducible.\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VosOpcpfGvWO"
      },
      "source": [
        "### Part 2.1a: Implement DCGAN (8 Points)\n",
        "Fill in the missing parts in the cells below in order to complete the Generator and Discriminator classes. You will need to define:\n",
        "\n",
        "- The hyperparameters\n",
        "- The constructors\n",
        "- `decode`\n",
        "- `discriminator`\n",
        "\n",
        "Recomendations for experimentation:\n",
        "- use the architecture that you implemented for the Autoencoder of Part 1 (encoder as discriminator, decoder as generator).\n",
        "- use the architecture desribed in the DCGAN paper (https://arxiv.org/abs/1511.06434).\n",
        "\n",
        "Some general reccomendations:\n",
        "- add several convolutional layers (3-4).\n",
        "- accelerate training with batch normalization after every convolutional layer.\n",
        "- use the appropriate activation functions. \n",
        "- Generator module: the upsampling can be done with various methods, such as nearest neighbor upsampling (`torch.nn.Upsample`) or transposed convolutions(`torch.nn.ConvTranspose2d`). \n",
        "- Discriminator module: Experiment with batch normalization (`torch.nn.BatchNorm2d`) and leaky relu (`torch.nn.LeakyReLu`) units after each convolutional layer.\n",
        "\n",
        "Try to follow the common practices for CNNs (e.g small kernels, max pooling, RELU activations), in order to narrow down your possible choices.\n",
        "\n",
        "<font color=\"red\">**Your model should not have more than 25 Million Parameters**</font>\n",
        "\n",
        "The number of epochs that will be needed in order to train the network will vary depending on your choices. As an advice, we recommend that while experimenting you should allow around 20 epochs and if the loss doesn't sufficiently drop, restart the training with a more powerful architecture. You don't need to train the network to an extreme if you don't have the time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOi_Q_jleQJq"
      },
      "source": [
        "#### Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__ENlW2aeQJr"
      },
      "outputs": [],
      "source": [
        "batch_size =   # change that\n",
        "\n",
        "transform = transforms.Compose([\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=mean, std=std),                        \n",
        "])\n",
        "# note - data_path was initialized at the top of the notebook\n",
        "cifar10_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform)\n",
        "cifar10_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n",
        "loader_train = DataLoader(cifar10_train, batch_size=batch_size)\n",
        "loader_test = DataLoader(cifar10_test, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TmrRudFRhOB"
      },
      "source": [
        "We'll visualize a subset of the test set: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY2ka775Rfxm"
      },
      "outputs": [],
      "source": [
        "samples, _ = next(iter(loader_test))\n",
        "\n",
        "samples = samples.cpu()\n",
        "samples = make_grid(denorm(samples), nrow=8, padding=2, normalize=False,\n",
        "                        value_range=None, scale_each=False, pad_value=0)\n",
        "plt.figure(figsize = (15,15))\n",
        "plt.axis('off')\n",
        "show(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSkveP0mZ-Pd"
      },
      "source": [
        "#### Model Definition\n",
        "Define hyperparameters and the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBoxMTihZ-Pd"
      },
      "outputs": [],
      "source": [
        "# *CODE FOR PART 2.1 IN THIS CELL*\n",
        "\n",
        "# Choose the number of epochs, the learning rate\n",
        "# and the size of the Generator's input noise vetor.\n",
        "\n",
        "num_epochs = \n",
        "learning_rate =\n",
        "latent_vector_size = \n",
        "\n",
        "# Other hyperparams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATTUAhCDZ-Pg"
      },
      "outputs": [],
      "source": [
        "# *CODE FOR PART 2.1 IN THIS CELL*\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        #######################################################################\n",
        "        #                       ** START OF YOUR CODE **\n",
        "        #######################################################################\n",
        "\n",
        "        #######################################################################\n",
        "        #                       ** END OF YOUR CODE **\n",
        "        ####################################################################### \n",
        "\n",
        "    # You can modify the arguments of this function if needed\n",
        "    def forward(self, z):\n",
        "        #######################################################################\n",
        "        #                       ** START OF YOUR CODE **\n",
        "        #######################################################################\n",
        "       \n",
        "        #######################################################################\n",
        "        #                       ** END OF YOUR CODE **\n",
        "        #######################################################################\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        #######################################################################\n",
        "        #                       ** START OF YOUR CODE **\n",
        "        #######################################################################\n",
        "     \n",
        "        #######################################################################\n",
        "        #                       ** END OF YOUR CODE **\n",
        "        ####################################################################### \n",
        "        \n",
        "    # You can modify the arguments of this function if needed\n",
        "    def forward(self, x):\n",
        "        #######################################################################\n",
        "        #                       ** START OF YOUR CODE **\n",
        "        #######################################################################\n",
        "     \n",
        "        #######################################################################\n",
        "        #                       ** END OF YOUR CODE **\n",
        "        ####################################################################### \n",
        "        \n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8YDyYf8Z-Pi"
      },
      "source": [
        "<h2> Initialize Model and print number of parameters </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh3NpfD_Z-Pj"
      },
      "source": [
        "You can use method `weights_init` to initialize the weights of the Generator and Discriminator networks. Otherwise, implement your own initialization, or do not use at all. You will not be penalized for not using initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAVpgpmUZ-Pk"
      },
      "outputs": [],
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ew-OdvNJZ-Pm"
      },
      "outputs": [],
      "source": [
        "use_weights_init = True\n",
        "\n",
        "model_G = Generator().to(device)\n",
        "if use_weights_init:\n",
        "    model_G.apply(weights_init)\n",
        "params_G = sum(p.numel() for p in model_G.parameters() if p.requires_grad)\n",
        "print(\"Total number of parameters in Generator is: {}\".format(params_G))\n",
        "print(model_G)\n",
        "print('\\n')\n",
        "\n",
        "model_D = Discriminator().to(device)\n",
        "if use_weights_init:\n",
        "    model_D.apply(weights_init)\n",
        "params_D = sum(p.numel() for p in model_D.parameters() if p.requires_grad)\n",
        "print(\"Total number of parameters in Discriminator is: {}\".format(params_D))\n",
        "print(model_D)\n",
        "print('\\n')\n",
        "\n",
        "print(\"Total number of parameters is: {}\".format(params_G + params_D))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by_TNUPXJamb"
      },
      "source": [
        "### Part 2.1b: Training the Model (12 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00wgs1VNZ-Pp"
      },
      "source": [
        "#### Defining a Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPlxaL_cZ-Pq"
      },
      "outputs": [],
      "source": [
        "# You can modify the arguments of this function if needed\n",
        "def loss_function(out):\n",
        "    loss = \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrgmhlSXZ-Ps"
      },
      "source": [
        "<h3>Choose and initialize optimizers</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFM8iI24Z-Pt"
      },
      "outputs": [],
      "source": [
        "# setup optimizer\n",
        "# You are free to add a scheduler or change the optimizer if you want. We chose one for you for simplicity.\n",
        "beta1 = 0.5\n",
        "optimizerD = torch.optim.Adam(model_D.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
        "optimizerG = torch.optim.Adam(model_G.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ311RPlZ-Pv"
      },
      "source": [
        "<h3> Define fixed input vectors to monitor training and mode collapse. </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGB_9A1UZ-Pw"
      },
      "outputs": [],
      "source": [
        "fixed_noise = torch.randn(batch_size, latent_vector_size, 1, 1, device=device)\n",
        "# Additional input variables should be defined here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD9S_3yZZ-Py"
      },
      "source": [
        "#### Training Loop\n",
        "\n",
        "Complete the training loop below. We've defined some variables to keep track of things during training:\n",
        "* errD: Loss of Discriminator after being trained on real and fake instances\n",
        "* errG: Loss of Generator\n",
        "* D_x: Output of Discriminator for real images\n",
        "* D_G_z1: Output of Discriminator for fake images (When Generator is not being trained)\n",
        "* D_G_z2: Output of Discriminator for fake images (When Generator is being trained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5vD--X6Z-Pz"
      },
      "outputs": [],
      "source": [
        "train_losses_G = []\n",
        "train_losses_D = []\n",
        "\n",
        "# <- You may wish to add logging info here\n",
        "for epoch in range(num_epochs):\n",
        "    # <- You may wish to add logging info here\n",
        "    with tqdm.tqdm(loader_train, unit=\"batch\") as tepoch: \n",
        "        for i, data in enumerate(tepoch):\n",
        "            train_loss_D = 0\n",
        "            train_loss_G = 0\n",
        "            \n",
        "            #######################################################################\n",
        "            #                       ** START OF YOUR CODE **\n",
        "            #######################################################################\n",
        "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "\n",
        "            # train with real\n",
        "\n",
        "            # train with fake\n",
        "\n",
        "            # (2) Update G network: maximize log(D(G(z)))\n",
        "\n",
        "            ####################################################################### \n",
        "            #                       ** END OF YOUR CODE **\n",
        "            ####################################################################### \n",
        "            # Logging \n",
        "            if i % 50 == 0:\n",
        "                tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                tepoch.set_postfix(D_G_z=f\"{D_G_z1:.3f}/{D_G_z2:.3f}\", D_x=D_x,\n",
        "                                  Loss_D=errD.item(), Loss_G=errG.item())\n",
        "\n",
        "    if epoch == 0:\n",
        "        save_image(denorm(real_cpu.cpu()).float(), content_path/'CW_GAN/real_samples.png')\n",
        "    with torch.no_grad():\n",
        "        fake = model_G(fixed_noise)\n",
        "        save_image(denorm(fake.cpu()).float(), str(content_path/'CW_GAN/fake_samples_epoch_%03d.png') % epoch)\n",
        "    train_losses_D.append(train_loss_D / len(loader_train))\n",
        "    train_losses_G.append(train_loss_G / len(loader_train))\n",
        "    \n",
        "# save  models \n",
        "# if your discriminator/generator are conditional you'll want to change the inputs here\n",
        "torch.jit.save(torch.jit.trace(model_G, (fixed_noise)), content_path/'CW_GAN/GAN_G_model.pth')\n",
        "torch.jit.save(torch.jit.trace(model_D, (fake)), content_path/'CW_GAN/GAN_D_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOjxGURDINm7"
      },
      "source": [
        "## Part 2.1c: Results (10 Points)\n",
        "This part is fairly open-ended, but not worth too much so do not go crazy. The table below shows examples of what are considered good samples. Level 3 and above will get you 10/10 points, level 2 will roughly get you 5/10 points and level 1 and below will get you 0/10 points.\n",
        "\n",
        "<table><tr>\n",
        "<td> \n",
        "  <p align=\"center\">\n",
        "    <img alt=\"Routing\" src=\"https://drive.google.com/uc?id=18aWqRAnAVTRDY52y1yHSCdqSxUFRKOS9\" width=\"%30\">\n",
        "    <br>\n",
        "    <em style=\"color: grey\">Level 1</em>\n",
        "  </p> \n",
        "</td>\n",
        "<td> \n",
        "  <p align=\"center\">\n",
        "    <img alt=\"Routing\" src=\"https://drive.google.com/uc?id=1ymO2-jGAvWeUR2kaj_LxQcGYF1RWNRnw\" width=\"%30\">\n",
        "    <br>\n",
        "    <em style=\"color: grey\">Level 2</em>\n",
        "  </p> \n",
        "</td>\n",
        "<td> \n",
        "  <p align=\"center\">\n",
        "    <img alt=\"Routing\" src=\"https://drive.google.com/uc?id=13SW62ekW32NMYtfcdm_dCJJ3ZMOZEZAJ\" width=\"%30\">\n",
        "    <br>\n",
        "    <em style=\"color: grey\">Level 3</em>\n",
        "  </p> \n",
        "</td>\n",
        "</tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IQIKTdPZ-P5"
      },
      "source": [
        "### Generator samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIHi0HrJZ-P8"
      },
      "outputs": [],
      "source": [
        "input_noise = torch.randn(100, latent_vector_size, 1, 1, device=device)\n",
        "with torch.no_grad():\n",
        "    # visualize the generated images\n",
        "    generated = model_G(input_noise).cpu()\n",
        "    generated = make_grid(denorm(generated)[:100], nrow=10, padding=2, normalize=False, \n",
        "                        value_range=None, scale_each=False, pad_value=0)\n",
        "    plt.figure(figsize=(15,15))\n",
        "    save_image(generated, content_path/'CW_GAN/Teaching_final.png')\n",
        "    show(generated) # note these are now class conditional images columns rep classes 1-10\n",
        "\n",
        "it = iter(loader_test)\n",
        "sample_inputs, _ = next(it)\n",
        "fixed_input = sample_inputs[0:64, :, :, :]\n",
        "# visualize the original images of the last batch of the test set for comparison\n",
        "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
        "                value_range=None, scale_each=False, pad_value=0)\n",
        "plt.figure(figsize=(15,15))\n",
        "show(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tRtYjH_LbQ0"
      },
      "source": [
        "## Part 2.1d: Engineering Choices (10 Points)\n",
        "\n",
        "Discuss the process you took to arrive at your final architecture. This should include:\n",
        "\n",
        "* Which empirically useful methods did you utilize\n",
        "* What didn't work, what worked and what mattered most\n",
        "* Are there any tricks you came across in the literature etc. which you suspect would be helpful here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnFqwePhXeZ4"
      },
      "source": [
        "**Your Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC6ndLP5Z-P-"
      },
      "source": [
        "## Part 2.2: Understanding GAN Training (5 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz6oy7ixZ-P_"
      },
      "source": [
        "### Loss Curves\n",
        "**Your task:**\n",
        "\n",
        "\n",
        "Plot the losses curves for the discriminator $D$ and the generator $G$ as the training progresses and explain whether the produced curves are theoretically sensible and why this is (or not) the case (x-axis: epochs, y-axis: loss).\n",
        "\n",
        "Make sure that the version of the notebook you deliver includes these results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxrUDHfBZ-QA"
      },
      "outputs": [],
      "source": [
        "# ANSWER FOR PART 2.2 IN THIS CELL*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZCUIHKkS0kF"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "Do your loss curves look sensible? What would you expect to see and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYYOnd6YBN3k"
      },
      "source": [
        "**YOUR ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_WLkdSNZ-QE"
      },
      "source": [
        "## Part 2.3: Understanding Mode Collapse (5 points) \n",
        "**Your task:**\n",
        " \n",
        "Describe the what causes the phenomenon of Mode Collapse and how it may manifest in the samples from a GAN. \n",
        "\n",
        "Based on the images created by your generator using the `fixed_noise` vector during training, did you notice any mode collapse? what this behaviour may be attributed to, and what did you try to eliminate / reduce it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiPei-I0FCGM"
      },
      "outputs": [],
      "source": [
        "# Any additional code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpZuxPYUFedE"
      },
      "source": [
        "### Discussion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SynUx_QV7olI"
      },
      "source": [
        "**YOUR ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cud7gZw2M-0U"
      },
      "source": [
        "\n",
        "\n",
        "# TA Test Cell\n",
        "TAs will run this cell to ensure that your results are reproducible, and that your models have been defined suitably. \n",
        "\n",
        "<font color=\"orange\"> <b> Please provide the input and output transformations required to make your VAE and GANs work. If your GAN generator requires more than just noise as input, also specify this below (there are two marked cells for you to inspect) </b></font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JesvipjEFbGx"
      },
      "outputs": [],
      "source": [
        "# If you want to run these tests yourself, change directory:\n",
        "# %cd '.../dl_cw2/'\n",
        "ta_data_path = \"../data\" # You can change this to = data_path when testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLlCRIn6m9ZS"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUmBbya2nQh9",
        "outputId": "2ae37d0c-d6ed-4eaf-c548-37c554bb4735"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb1d90f4bd0>"
            ]
          },
          "execution_count": 9,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Do not remove anything here\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, sampler\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "show = lambda img: plt.imshow(np.transpose(img.cpu().numpy(), (1,2,0)))\n",
        "\n",
        "device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Do not change this cell!\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFfBZsnTzQIU"
      },
      "outputs": [],
      "source": [
        "############# CHANGE THESE (COPY AND PASTE FROM YOUR OWN CODE) #############\n",
        "vae_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def vae_denorm(x):\n",
        "    return x\n",
        "\n",
        "def gan_denorm(x):\n",
        "    return x\n",
        "\n",
        "gan_latent_size = \n",
        "\n",
        "# If your generator requires something other than noise as input, please specify\n",
        "# two cells down from here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t0CVMCFyxgU"
      },
      "outputs": [],
      "source": [
        "# Load VAE Dataset\n",
        "test_dat = datasets.MNIST(ta_data_path, train=False, transform=vae_transform, \n",
        "                          download=True)\n",
        "vae_loader_test = DataLoader(test_dat, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTNsHqK-mh6U"
      },
      "outputs": [],
      "source": [
        "############# MODIFY IF NEEDED #############\n",
        "vae_input, _ = next(iter(vae_loader_test))\n",
        "\n",
        "# If your generator is conditional, then please modify this input suitably\n",
        "input_noise = torch.randn(100, gan_latent_size, 1, 1, device=device)\n",
        "gan_input = [input_noise] # In case you want to provide a tuple, we wrap ours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIg06rOqo3XA"
      },
      "outputs": [],
      "source": [
        "# VAE Tests\n",
        "# TAs will change these paths as you will have provided the model files manually\n",
        "\"\"\"To TAs, you should have been creating a folder with the student uid\n",
        "   And the .ipynb + models in the root. Then that path is './VAE_model.pth' etc.\n",
        "\"\"\"\n",
        "vae = model_G = torch.jit.load('./CW_VAE/VAE_model.pth')\n",
        "vae.eval()\n",
        "\n",
        "# Check if VAE is convolutional\n",
        "def recurse_cnn_check(parent, flag):\n",
        "    if flag:\n",
        "        return flag\n",
        "    children = list(parent.children())\n",
        "    if len(children) > 0:\n",
        "        for child in children:\n",
        "            flag = flag or recurse_cnn_check(child, flag)\n",
        "    else:\n",
        "        params = parent._parameters\n",
        "        if 'weight' in params.keys():\n",
        "            flag = params['weight'].ndim == 4\n",
        "    return flag\n",
        "\n",
        "has_cnn = recurse_cnn_check(vae, False)\n",
        "print(\"Used CNN\" if has_cnn else \"Didn't Use CNN\")\n",
        "\n",
        "vae_in = make_grid(vae_denorm(vae_input), nrow=8, padding=2, normalize=False,\n",
        "                value_range=None, scale_each=False, pad_value=0)\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "show(vae_in)\n",
        "\n",
        "vae_test = vae(vae_input.to(device))[0].detach()\n",
        "vae_reco = make_grid(vae_denorm(vae_test), nrow=8, padding=2, normalize=False,\n",
        "                value_range=None, scale_each=False, pad_value=0)\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "show(vae_reco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoJLGFZSo7ON"
      },
      "outputs": [],
      "source": [
        "# GAN Tests\n",
        "model_G = torch.jit.load('./CW_GAN/GAN_G_model.pth')\n",
        "model_D = torch.jit.load('./CW_GAN/GAN_D_model.pth')\n",
        "[model.eval() for model in (model_G, model_D)]  \n",
        "\n",
        "# Check that GAN doesn't have too many parameters\n",
        "num_param = sum(p.numel() for p in [*model_G.parameters(),*model_D.parameters()])\n",
        "\n",
        "print(f\"Number of Parameters is {num_param} which is\", \"ok\" if num_param<25E+6 else \"not ok\")\n",
        "\n",
        "# visualize the generated images\n",
        "generated = model_G(*gan_input).cpu()\n",
        "generated = make_grid(gan_denorm(generated)[:100].detach(), nrow=10, padding=2, normalize=False, \n",
        "                    value_range=None, scale_each=False, pad_value=0)\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.axis('off')\n",
        "show(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9H91RxRuQm1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
